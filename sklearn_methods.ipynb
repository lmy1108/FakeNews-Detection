{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "df.tail()\n",
    "\n",
    "#替换列名\n",
    "df.rename(columns={\"label\":\"fake\"}, inplace=True)\n",
    "label_map = {\"FAKE\": 1, \"REAL\": 0}\n",
    "df['fake'] = df['fake'].map(label_map)\n",
    "\n",
    "\n",
    "#丢掉无用的列\n",
    "df = df.drop(['Unnamed: 0','title_vectors'],axis=1)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#正则表达式去掉一些无用的形式\n",
    "def execute(x):\n",
    "    temp=x\n",
    "    pat1 = \"[a-zA-Z]+'t\"\n",
    "    #substitude the ***'t' words with not ***\n",
    "    temp = re.sub(pat1, 'not', temp)\n",
    "    pat2 = \"[a-zA-Z]+’t\"\n",
    "\n",
    "    temp = re.sub(pat2, 'not', temp)\n",
    "    #邮箱地址\n",
    "    pat3=\"[\\w!#$%&'*+/=?^_`{|}~-]+(?:\\.[\\w!#$%&'*+/=?^_`{|}~-]+)*@(?:[\\w](?:[\\w-]*[\\w])?\\.)+[\\w](?:[\\w-]*[\\w])?\"\n",
    "    temp = re.sub(pat3, 'email', temp)\n",
    "    #url网址\n",
    "    pat4=\"[a-zA-z]+://[^\\s]*\"\n",
    "    temp = re.sub(pat4, 'url', temp)\n",
    "    #日期\n",
    "    pat5=\"([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8])))\"\n",
    "    temp = re.sub(pat5, 'date', temp)\n",
    "    #电话号码 qq号码等 各种数字串\n",
    "    pat6=\"[0-9]+\"\n",
    "    temp = re.sub(pat6, 'number', temp)\n",
    "    return temp\n",
    "\n",
    "def text_execute(x):\n",
    "    return execute(x['text'])\n",
    "\n",
    "def title_execute(x):\n",
    "    return execute(x['title'])\n",
    "    \n",
    "\n",
    "df['text'] = df.apply(text_execute,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def splitSentence(paragraph):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(paragraph)\n",
    "    return len(sentences)\n",
    "\n",
    "df['numerics']=df['text'].apply(lambda sen:len([x for x in sen.split() if x.isdigit()]))\n",
    "df['text'] = df.apply(text_execute,axis=1)\n",
    "df['sentence_count']=df['text'].apply(splitSentence)\n",
    "df['word_count']=df['text'].apply(lambda x:len(str(x).split(\" \")))\n",
    "df['char_count']=df['text'].str.len()\n",
    "df['avg_word']=df.apply(lambda x:x['char_count']/x['word_count'],axis=1)\n",
    "#df['avg_sentence']=df.apply(lambda x:x['word_count']/x['sentence_count'],axis=1)\n",
    "stop=stopwords.words('english')\n",
    "df['stopwords']=df['text'].apply(lambda sen:len([x for x in sen.split() if x in stop]))\n",
    "df['upper']=df['text'].apply(lambda sen:len([x for x in sen.split() if x.isupper()]))\n",
    "df['hashtags']=df['text'].apply(lambda sen:len([x for x in sen.split() if x.startswith(\"#\") or  x.startswith(\"$\") or  x.startswith(\"&\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "Here we use textblob for sentiment analysis, and the range of polarity is -1 to 1, the range of subjectivity is from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "df['sentiment_polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "df['sentiment_subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fake</th>\n",
       "      <th>numerics</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>upper</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>1298</td>\n",
       "      <td>7513</td>\n",
       "      <td>5.788136</td>\n",
       "      <td>534</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059595</td>\n",
       "      <td>0.562654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>446</td>\n",
       "      <td>2641</td>\n",
       "      <td>5.921525</td>\n",
       "      <td>176</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082652</td>\n",
       "      <td>0.518638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>423</td>\n",
       "      <td>2549</td>\n",
       "      <td>6.026005</td>\n",
       "      <td>173</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102574</td>\n",
       "      <td>0.348775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November number, n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>404</td>\n",
       "      <td>2715</td>\n",
       "      <td>6.720297</td>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.503563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>307</td>\n",
       "      <td>1850</td>\n",
       "      <td>6.026059</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251709</td>\n",
       "      <td>0.420109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text  fake  numerics  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...     1         0   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...     1         0   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...     0         0   \n",
       "3  — Kaydee King (@KaydeeKing) November number, n...     1         0   \n",
       "4  It's primary day in New York and front-runners...     0         0   \n",
       "\n",
       "   sentence_count  word_count  char_count  avg_word  stopwords  upper  \\\n",
       "0              87        1298        7513  5.788136        534     38   \n",
       "1              26         446        2641  5.921525        176      3   \n",
       "2              16         423        2549  6.026005        173      6   \n",
       "3              17         404        2715  6.720297        127      5   \n",
       "4              21         307        1850  6.026059        120      3   \n",
       "\n",
       "   hashtags  sentiment_polarity  sentiment_subjectivity  \n",
       "0         0            0.059595                0.562654  \n",
       "1         0            0.082652                0.518638  \n",
       "2         0            0.102574                0.348775  \n",
       "3         0            0.063645                0.503563  \n",
       "4         0            0.251709                0.420109  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "\n",
    "titles = df.title.values\n",
    "texts = df.text.values\n",
    "y = df.fake.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_text_word = []\n",
    "for i in range(0, len(df)):\n",
    "    title_text_word.append(str(titles[i]) + \" \" + str(texts[i]))\n",
    "tokenized = [nltk.word_tokenize(word) for word in title_text_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of both the most frequent words before and after the data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.  the  freq:  292303\n",
      " 2.    ,  freq:  261049\n",
      " 3.    .  freq:  204791\n",
      " 4.   to  freq:  140937\n",
      " 5.   of  freq:  130644\n",
      " 6.  and  freq:  119928\n",
      " 7.    a  freq:  108544\n",
      " 8.   in  freq:   99154\n",
      " 9. that  freq:   72460\n",
      "10.number  freq:   57140\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "token_counter = Counter(token.lower() for sentences in tokenized for token in sentences)\n",
    "top10 = token_counter.most_common()[:10]\n",
    "for index, tok in enumerate(top10):\n",
    "    print('{:>2}.{:>5}  freq: {:>7}'.format(index+1, tok[0], tok[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.number  freq:   57140\n",
      " 2.trump  freq:   22985\n",
      " 3. said  freq:   21176\n",
      " 4.clinton  freq:   17946\n",
      " 5.would  freq:   12751\n",
      " 6.people  freq:   11668\n",
      " 7.  one  freq:   11381\n",
      " 8.  new  freq:    9542\n",
      " 9.state  freq:    8874\n",
      "10.president  freq:    8615\n",
      "11.obama  freq:    8496\n",
      "12. also  freq:    8216\n",
      "13.campaign  freq:    7806\n",
      "14.   us  freq:    7732\n",
      "15.hillary  freq:    7707\n",
      "16. like  freq:    7095\n",
      "17.could  freq:    6657\n",
      "18. time  freq:    6487\n",
      "19. even  freq:    6454\n",
      "20.states  freq:    6184\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def clean_text(tokenized_list, stopwords, punctuation, lemmatize=False):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc \n",
    "                         if token.lower() not in chain(stopwords, punctuation) \n",
    "                         and token.lower() != \"'s\" and token.lower() != \"''\" \n",
    "                         and token.lower() != \"``\" and token.lower() != \"—\"])\n",
    "        # delete the expression like \"'s\"\n",
    "    return new_list\n",
    "stop_word = stopwords.words('english')\n",
    "punct = punctuation + '’' + '‘' + '”' + '“'\n",
    "cleaned_list = clean_text(tokenized, stop_word, punct)\n",
    "# re-counter the token and list the new top10 most common words.\n",
    "new_token_counter = Counter(token.lower() for sentences in cleaned_list for token in sentences)\n",
    "new_top20 = new_token_counter.most_common()[:20]\n",
    "for index, tok in enumerate(new_top20):\n",
    "    print('{:>2}.{:>5}  freq: {:>7}'.format(index+1, tok[0], tok[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Gensim Doc2Vec (300 dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "class TagDocIterator:\n",
    "    def __init__(self, doc_list, idx_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.idx_list = idx_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc, idx, in zip(self.doc_list, self.idx_list):\n",
    "            tag = [idx]\n",
    "            yield TaggedDocument(words=doc, tags=tag)\n",
    "doc2vec_model = Doc2Vec(size=300, epoch=5, window=7,hs=1,dbow_words=1,dm=1,workers=4)\n",
    "doc2vec_model.build_vocab(TagDocIterator(cleaned_list, df.index))\n",
    "doc2vec_model.train(TagDocIterator(cleaned_list, df.index), epochs=10, total_examples=doc2vec_model.corpus_count)\n",
    "final_feature_text = []\n",
    "for i in range(len(doc2vec_model.docvecs)):\n",
    "    final_feature_text.append(doc2vec_model.docvecs[i])\n",
    "final_feature_title_1 = pd.DataFrame(final_feature_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the extra feature Vectors into the Trained Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_title_1['sentence_count']=df['sentence_count'].values\n",
    "final_feature_title_1['word_count']=df['word_count']\n",
    "final_feature_title_1['char_count']=df['char_count']\n",
    "final_feature_title_1['avg_word']=df['avg_word']\n",
    "final_feature_title_1['hashtags']=df['hashtags']\n",
    "final_feature_title_1['stopwords']=df['stopwords']\n",
    "final_feature_title_1['upper']=df['upper']\n",
    "final_feature_title_1['numerics']=df['numerics']\n",
    "final_feature_title_1['sentiment_polarity']=df['sentiment_polarity']\n",
    "final_feature_title_1['sentiment_subjectivity']=df['sentiment_subjectivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn to Nomarlize the vector, making sure they are in the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_title_1.tail()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "standard_text=ss.fit_transform(final_feature_title_1.values)\n",
    "final_feature = pd.DataFrame(standard_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4434, 310), (1901, 310), (4434,), (1901,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_1 = pd.concat([final_feature], axis=1)\n",
    "Y_1 = df['fake'].as_matrix()\n",
    "train_x_1, test_x_1, train_y_1, test_y_1 = train_test_split(train_1, Y_1, test_size=0.3, random_state=1)\n",
    "train_x_1.shape, test_x_1.shape, train_y_1.shape, test_y_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Naive Doc2Vec\n",
    "\n",
    "After building the Doc2vec, we did some experienments to judge which strategy we should choose when we are processin the data:\n",
    "These stratagy include : Whether we should remove the stop rare word , whether we should remove punctuations,Whether adding the new features help with accuracy.\n",
    "and ways to deal with the titles ,we canconcatenate titles with the text, and we can also build an indivisual title text. \n",
    "Also, we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also used the word2vec to build the naive doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from gensim import models\n",
    "# Training word2vec model on Gutenberg corpus. This may take a few minutes.\n",
    "model = models.Word2Vec(cleaned_list,\n",
    "                        size = 300,\n",
    "                        window = 9,\n",
    "                        min_count = 1,\n",
    "                        sg = 1,\n",
    "                        alpha = 0.025,\n",
    "                        iter=10,\n",
    "                        batch_words = 10000,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "new_title_vectors_1 = np.zeros((len(df), 300))\n",
    "for i in range(0, len(df)):\n",
    "    # I find that a line has no title after we cleaned the punctuation and stopwords, so the length maybe 0\n",
    "    if len(cleaned_list[i]) != 0:\n",
    "        for word in cleaned_list[i]:\n",
    "            new_title_vectors_1[i] += model.wv[word]\n",
    "        # calculate the average of the word vector\n",
    "        new_title_vectors_1[i] = new_title_vectors_1[i] / len(cleaned_list[i])\n",
    "new_title_vectors_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "final_feature_w2v = pd.DataFrame(new_title_vectors_1)\n",
    "train_1 = pd.concat([final_feature_w2v], axis=1)\n",
    "Y_1 = df['fake'].as_matrix()\n",
    "\n",
    "train_x_1, test_x_1, train_y_1, test_y_1 = train_test_split(train_1, Y_1, test_size=0.3, random_state=1)\n",
    "train_x_1.shape, test_x_1.shape, train_y_1.shape, test_y_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the two(Gensim Doc2Vec, Naive Doc2Vec together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we concatenate the two vectors into a larger vector with more than 600 dimensions the result will get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_concat1 = pd.concat([final_feature_w2v,final_feature],axis=1)\n",
    "final_feature_concat = pd.DataFrame(final_feature_concat1.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_1 = pd.concat([final_feature_concat], axis=1)\n",
    "Y_1 = df['fake'].as_matrix()\n",
    "\n",
    "train_x_1, test_x_1, train_y_1, test_y_1 = train_test_split(train_1, Y_1, test_size=0.3, random_state=1)\n",
    "train_x_1.shape, test_x_1.shape, train_y_1.shape, test_y_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGmodel = XGBClassifier(max_depth=7, \n",
    "                        learning_rate=0.2, \n",
    "                        n_estimators=1000,\n",
    "                        silent=True, \n",
    "                        objective='binary:logistic', \n",
    "                        nthread=-1, \n",
    "                        gamma=0,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0, \n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1, \n",
    "                        colsample_bylevel=1, \n",
    "                        reg_alpha=0, \n",
    "                        reg_lambda=1, \n",
    "                        scale_pos_weight=1, \n",
    "                        base_score=0.5,\n",
    "                        seed=0, \n",
    "                        missing=None)\n",
    "XGmodel.fit(train_x_1, train_y_1)\n",
    "y_pred = XGmodel.predict(test_x_1)\n",
    "y_pred= (y_pred>0.5)\n",
    "print(metrics.accuracy_score(test_y_1, y_pred),'\\n')\n",
    "\n",
    "matrix = metrics.confusion_matrix(test_y_1,y_pred)\n",
    "print(matrix,'\\n')\n",
    "\n",
    "print(classification_report(test_y_1, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "lr_model_1 = LogisticRegression(max_iter=60)\n",
    "lr_model_1.fit(train_x_1, train_y_1)\n",
    "training_time = time.time() - start\n",
    "predict_y_1 = lr_model_1.predict(test_x_1)\n",
    "accuracy_1 = metrics.accuracy_score(test_y_1.ravel(), predict_y_1.ravel())\n",
    "precision_1 = metrics.precision_score(test_y_1.ravel(), predict_y_1.ravel())\n",
    "recall_1 = metrics.recall_score(test_y_1.ravel(), predict_y_1.ravel())\n",
    "f1_1 = metrics.f1_score(test_y_1.ravel(), predict_y_1.ravel())\n",
    "print('Accuracy: {:.2f} | Precision: {:.2f} | Recall: {:.2f} | F1-measure: {:.2f} | Training time: {:.2f}s'\n",
    "      .format(accuracy_1, precision_1, recall_1, f1_1, training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "x_train=train_x_1\n",
    "x_test=test_x_1\n",
    "y_train=train_y_1\n",
    "y_test=test_y_1\n",
    "mlp=MLPClassifier()\n",
    "\n",
    "y_pred = mlp.fit(x_train, y_train).predict(x_test)\n",
    "accuracy = metrics.accuracy_score(y_test.ravel(), y_pred.ravel())\n",
    "matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print('Accuracy= {:.4f}'.format(accuracy))\n",
    "print('\\nconfusion_matrix:\\n',matrix)\n",
    "print('report:\\n',classification_report(y_test, y_pred))\n",
    "# print('Accuracy= {:.4f} | Precision= {:.4f} | Recall= {:.4f} | f1 score= {:.4f}'.format(accuracy,precision,recall,f1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('xgb', XGmodel), ('lr_model_1', lr_model_1),('mlp',mlp)],weights=[3,1,1])\n",
    "pre=eclf.fit(x_train, y_train)\n",
    "y_pred = eclf.predict(x_test)\n",
    "y_pred= (y_pred>0.5)\n",
    "print(metrics.accuracy_score(y_test, y_pred),'\\n')\n",
    "\n",
    "matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(matrix,'\\n')\n",
    "\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,ngram_range=(1,2),max_df= 0.85, min_df= 0.01,sublinear_tf=True, use_idf=True)\n",
    "count = CountVectorizer(stop_words=ENGLISH_STOP_WORDS,ngram_range=(1,2),max_df= 0.85, min_df= 0.01)\n",
    "svd = TruncatedSVD(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_body_text=texts\n",
    "X_headline_text=titles\n",
    "X_body_tfidf = tfidf.fit_transform(X_body_text)\n",
    "X_headline_tfidf = tfidf.fit_transform (X_headline_text)\n",
    "\n",
    "X_body_count = count.fit_transform(X_body_text)\n",
    "X_headline_count = count.fit_transform (X_headline_text)\n",
    "\n",
    "X_SVDtransformed = svd.fit_transform(X_body_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_body_tfidf_train, X_body_tfidf_test, y_body_train, y_body_test = train_test_split(X_body_tfidf,y, test_size = 0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_body_count_train, X_body_count_test, y_body_train, y_body_test = train_test_split(X_body_count,y, test_size = 0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_svd_train, X_svd_test, y_svd_train, y_svd_test = train_test_split(X_body_tfidf,y, test_size = 0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_count = LogisticRegression(penalty='l1')\n",
    "# train model\n",
    "lr_count.fit(X_body_count_train, y_body_train)\n",
    "# get predictions for article section\n",
    "y_body_pred = lr_count.predict(X_body_count_test)\n",
    "# print metrics\n",
    "print (\"Logistig Regression F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%\".format( f1_score(y_body_test, y_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(y_body_test, y_body_pred)*100) )\n",
    "cros_val_list = cross_val_score(lr_count, X_body_count,y,cv=7)\n",
    "print (cros_val_list)\n",
    "print (cros_val_list.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression + raw count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_count = LogisticRegression(penalty='l1')\n",
    "# train model\n",
    "lr_count.fit(X_body_count_train, y_body_train)\n",
    "# get predictions for article section\n",
    "y_body_pred = lr_count.predict(X_body_count_test)\n",
    "# print metrics\n",
    "print (\"Logistig Regression F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%\".format( f1_score(y_body_test, y_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(y_body_test, y_body_pred)*100) )\n",
    "cros_val_list = cross_val_score(lr_count, X_body_count,y,cv=7)\n",
    "print (cros_val_list)\n",
    "print (cros_val_list.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost + tfidf count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_tfidf = XGBClassifier(max_depth=5, \n",
    "                        learning_rate=0.2, \n",
    "                        n_estimators=1000,\n",
    "                        silent=True, \n",
    "                        objective='binary:logistic', \n",
    "                        nthread=-1, \n",
    "                        gamma=0,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0, \n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1, \n",
    "                        colsample_bylevel=1, \n",
    "                        reg_alpha=0, \n",
    "                        reg_lambda=1, \n",
    "                        scale_pos_weight=1, \n",
    "                        base_score=0.5,\n",
    "                        seed=0, \n",
    "                        missing=None)\n",
    "xgb_tfidf.fit(X_body_tfidf_train, y_body_train)\n",
    "y_xgb_body_pred = xgb_tfidf.predict(X_body_tfidf_test)\n",
    "# print metrics\n",
    "print (\"XGBoost F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%``\".format( f1_score(y_body_test, y_xgb_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(y_body_test, y_xgb_body_pred)*100) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPClassifier + svd+tfidf count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_svd=MLPClassifier()\n",
    "y_pred = mlp_svd.fit(X_svd_train, y_body_train).predict(X_svd_test)\n",
    "accuracy = metrics.accuracy_score(y_body_test.ravel(), y_pred.ravel())\n",
    "matrix = metrics.confusion_matrix(y_body_test,y_pred)\n",
    "\n",
    "print('Accuracy= {:.4f}'.format(accuracy))\n",
    "print('\\nconfusion_matrix:\\n',matrix)\n",
    "print('report:\\n',classification_report(y_body_test, y_pred))\n",
    "# print('Accuracy= {:.4f} | Precision= {:.4f} | Recall= {:.4f} | f1 score= {:.4f}'.format(accuracy,precision,recall,f1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('xgb_tfidf', xgb_tfidf), ('lr_count', lr_count),('mlp_svd',mlp_svd)],weights=[1,1,2])\n",
    "pre=eclf2.fit(x_train, y_train)\n",
    "y_pred = eclf2.predict(x_test)\n",
    "y_pred= (y_pred>0.5)\n",
    "print(metrics.accuracy_score(y_test, y_pred),'\\n')\n",
    "\n",
    "matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(matrix,'\\n')\n",
    "\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "\n",
    "model_fasttext = FastText(cleaned_list, size=300, window=7, min_count=1, iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "fasttext_vectors = np.zeros((len(df),300))\n",
    "for i in range(0, len(df)):\n",
    "    # I find that a line has no title after we cleaned the punctuation and stopwords, so the length maybe 0\n",
    "    if len(cleaned_list[i]) != 0:\n",
    "        for word in cleaned_list[i]:\n",
    "            fasttext_vectors[i] += model_fasttext.wv[word]\n",
    "        # calculate the average of the word vector\n",
    "        fasttext_vectors[i] = fasttext_vectors[i] / len(cleaned_list[i])\n",
    "fasttext_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_fast = pd.DataFrame(fasttext_vectors)\n",
    "Y_1 = df['fake'].as_matrix()\n",
    "train_x_fast, test_x_fast, train_y_fast, test_y_fast = train_test_split(train_fast, Y_1, test_size=0.3, random_state=1)\n",
    "train_x_fast.shape, test_x_fast.shape, train_y_fast.shape, test_y_fast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "lr_model_fast = LogisticRegression(max_iter=60)\n",
    "lr_model_fast.fit(train_x_fast, train_y_fast)\n",
    "training_time = time.time() - start\n",
    "predict_y_1 = lr_model_fast.predict(test_x_fast)\n",
    "accuracy_1 = metrics.accuracy_score(test_y_fast.ravel(), predict_y_1.ravel())\n",
    "precision_1 = metrics.precision_score(test_y_fast.ravel(), predict_y_1.ravel())\n",
    "recall_1 = metrics.recall_score(test_y_fast.ravel(), predict_y_1.ravel())\n",
    "f1_1 = metrics.f1_score(test_y_fast.ravel(), predict_y_1.ravel())\n",
    "print('Accuracy: {:.2f} | Precision: {:.2f} | Recall: {:.2f} | F1-measure: {:.2f} | Training time: {:.2f}s'\n",
    "      .format(accuracy_1, precision_1, recall_1, f1_1, training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_fasttext=MLPClassifier()\n",
    "y_pred = mlp_fasttext.fit(train_x_fast, train_y_fast).predict(test_x_fast)\n",
    "accuracy = metrics.accuracy_score(test_y_fast.ravel(), y_pred.ravel())\n",
    "matrix = metrics.confusion_matrix(test_y_fast,y_pred)\n",
    "\n",
    "print('Accuracy= {:.4f}'.format(accuracy))\n",
    "print('\\nconfusion_matrix:\\n',matrix)\n",
    "print('report:\\n',classification_report(test_y_fast, y_pred))\n",
    "# print('Accuracy= {:.4f} | Precision= {:.4f} | Recall= {:.4f} | f1 score= {:.4f}'.format(accuracy,precision,recall,f1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "xg_test = XGBClassifier(max_depth=7, \n",
    "                        learning_rate=0.2, \n",
    "                        n_estimators=1000,\n",
    "                        silent=True, \n",
    "                        objective='binary:logistic', \n",
    "                        nthread=-1, \n",
    "                        gamma=0,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0, \n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1, \n",
    "                        colsample_bylevel=1, \n",
    "                        reg_alpha=0, \n",
    "                        reg_lambda=1, \n",
    "                        scale_pos_weight=1, \n",
    "                        base_score=0.5,\n",
    "                        seed=0, \n",
    "                        missing=None)\n",
    "xg_test.fit(train_x_fast, train_y_fast)\n",
    "y_pred = xg_test.predict(test_x_fast)\n",
    "y_pred= (y_pred>0.5)\n",
    "print(metrics.accuracy_score(test_y_fast, y_pred),'\\n')\n",
    "\n",
    "matrix = metrics.confusion_matrix(test_y_fast,y_pred)\n",
    "print(matrix,'\\n')\n",
    "\n",
    "print(classification_report(test_y_fast, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9484481851657023 \n",
      "\n",
      "[[871  43]\n",
      " [ 55 932]] \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.95      0.95       914\n",
      "          1       0.96      0.94      0.95       987\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf_final = VotingClassifier(estimators=[('eclf', eclf), ('eclf2', eclf2)],weights=[2,1])\n",
    "pre=eclf_final.fit(x_train, y_train)\n",
    "y_pred = eclf_final.predict(x_test)\n",
    "y_pred= (y_pred>0.5)\n",
    "print(metrics.accuracy_score(y_test, y_pred),'\\n')\n",
    "\n",
    "matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(matrix,'\\n')\n",
    "\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
